---
name: Update arxiv.org stats
on:
  #schedule:
    # https://crontab.guru/every-day
    #- cron: "0 0 * * *"
  workflow_dispatch:
env:
  CSV_FILE: data/dummy-data.csv
jobs:
  updateArxivCsv:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4
        with:
          ref: dev-arxiv-fetch-parse
      #    sparse-checkout: ${{ env.OUT_FILE }}
      #    sparse-checkout-cone-mode: false
      #- uses: actions/setup-python@v5
        # https://github.com/actions/setup-python
      #  with:
      #    python-version: '3.10'
#      - name: fetch statistics for arxiv.org
#        # https://github.com/karpathy/arxiv-sanity-lite/blob/master/aslite/arxiv.py#L38
#        shell: python
#        env:
#          base_url: 'http://export.arxiv.org/api/query?'
#          add_url: 'search_query=%s&sortBy=lastUpdatedDate&start=%d&max_results=100'
#        run: |
#          $add_url % (search_query, start_index)
      #- name: download data from arxiv API
      - name: save data to csv
        shell: python
        run: |
          import csv
          from random import random, choice, seed
          from datetime import datetime, timezone
          seed(a=24, version=2)
          id = random() * 3
          version = ''.join(choice('0123456789ABCDEF') for i in range(8))
          time = datetime.now(timezone.utc)
          time_str = datetime.now(timezone.utc).timestamp() * 1000
          header = ["id", "version", "time", "time_str"]
          data = [id, version, time, time_str]
          with open("${{ env.CSV_FILE }}", 'w', newline='') as f: # encoding='UTF8'
            writer = csv.writer(f)
            writer.writerow(header)
            writer.writerow(data)
      - name: push updated csv file back to main branch
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add $CSV_FILE
          git commit -m "Updated $CSV_FILE"
          git push
...
