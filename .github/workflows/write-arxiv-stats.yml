---
name: Update arxiv.org stats
on:
  #schedule:
    # https://crontab.guru/every-day
    #- cron: "0 0 * * *"
  workflow_dispatch:
env:
  CSV_FILE: data/dummy-data.csv
  QUERY:  'cat:cs.CV+OR+cat:cs.LG+OR+cat:cs.CL+OR+cat:cs.AI+OR+cat:cs.NE+OR+cat:cs.RO'
jobs:
  updateArxivCsv:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4
        with:
          ref: dev-arxiv-fetch-parse
      #    sparse-checkout: ${{ env.OUT_FILE }}
      #    sparse-checkout-cone-mode: false
      #- uses: actions/setup-python@v5
        # https://github.com/actions/setup-python
      #  with:
      #    python-version: '3.10'
      #    cache: 'pip' # caching pip dependencies
      - run: pip install -m 'feedparser==6.0.8' -Uqq # -r requirements.txt
      - name: fetch and parse statistics for arxiv.org
        # https://github.com/karpathy/arxiv-sanity-lite/blob/d7a303b410b0246fbd19087e37f1885f7ca8a9dc/aslite/arxiv.py#L38
        shell: python
        run: |
          # fetch and parse
          import feedparser
          import urllib.request
          # write to csv
          import csv
          #from random import random, choice, seed
          #from datetime import datetime, timezone
          # https://github.com/karpathy/arxiv-sanity-lite/blob/d7a303b410b0246fbd19087e37f1885f7ca8a9dc/aslite/arxiv.py#L15
          base_url = 'http://export.arxiv.org/api/query?'
          add_url = 'search_query=%s&sortBy=lastUpdatedDate&start=%d&max_results=100' % (search_query, start_index)
          #add_url = 'search_query=%s&sortBy=submittedDate&start=%d&max_results=100' % (search_query, start_index)
          search_query = base_url + add_url
          with urllib.request.urlopen(search_query) as url:
              response = url.read()
          if url.status != 200:
              print(f"arxiv did not return status 200 response")
          out = []
          parse = feedparser.parse(response)
          for e in parse.entries:
              j = encode_feedparser_dict(e)
              # extract / parse id information
              idv, rawid, version = parse_arxiv_url(j['id'])
              j['_idv']= idv
              j['_id'] = rawid
              j['_version'] = version
              j['_time'] = time.mktime(j['updated_parsed'])
              j['_time_str'] = time.strftime('%b %d %Y', j['updated_parsed'])
              # delete apparently spurious and redundant information
              del j['summary_detail']
              del j['title_detail']
              out.append(j)
            # TODO retry n-times if connection failes
            # TODO sort descending and check if paper already id present    
          with open("${{ env.CSV_FILE }}", 'w', newline='') as f: # encoding='UTF8'
            writer = csv.writer(f)
            writer.writerow(header)
            for o in out:
              writer.writerow(o)
      - name: push updated csv file back to main branch
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add $CSV_FILE
          git commit -m "Updated $CSV_FILE"
          git push
...
